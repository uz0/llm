# Архитектура и тестирование агентов. Сергей Парфенов

BNS: agent architecture and testing. by Sergei Parfenov (2025)

Это обзор и сравнение популярных архитектур языковых агентов, используемых при решении сложных задач с участием рассуждений, планирования, инструментов и рефлексии. Материал основан на научных статьях и публичных экспериментах.

## Обзор архитектур

| Архитектура               | Ключевая идея                       |
| ------------------------- | ----------------------------------- |
| Базовое размышление       | Один проход генерации ответа        |
| Актор–рефлектор           | Итеративная самокритика и улучшение |
| Tree of Thoughts          | Поиск решений в виде дерева         |
| Планирование и выполнение | Явный пошаговый план                |
| ReWOO                     | Разделение рассуждений и наблюдений |
| LLMCompiler               | DAG-оркестровка и параллелизм       |

---

## Описание архитектур

### Базовое размышление (Basic Reflection)

Основано на: _Reflexion: Language Agents with Verbal Reinforcement Learning_ (10/10/2023)

**Описание:**
Базовое размышление — это архитектура, в которой ответ формируется итеративно за счёт последовательной генерации, анализа и улучшения текста с использованием рефлексии.

**Поток работы:**

1. Пользовательский запрос поступает на вход.

2. Генерируется первоначальный ответ с использованием первого промпта (например, «написание эссе»).

3. Ответ передаётся во второй промпт для рефлексии (например, «оценка эссе»).

4. Этап рефлексии генерирует критику и идеи для улучшения.

5. Рефлексия и первоначальный ответ передаются обратно в исходный промпт для генерации переработанного черновика.

6. Процесс повторяется N раз, после чего результат возвращается пользователю.

**Ключевая особенность:**
Качество ответа повышается за счёт повторяющегося цикла генерации и самоанализа без использования внешних инструментов или явного планирования.

### Актор–рефлектор (Actor reflexion)

**Источник:** _Language Agents with Verbal Reinforcement Learning_ (10.10.2023)

**Поток работы:**

-   Пользовательский запрос поступает на вход.

-   Генерируется первоначальный ответ вместе с самокритикой и предложенными запросами к инструментам.

-   Предложенные запросы к инструментам выполняются (например, веб-поиск дополнительной информации).

-   Исходный ответ, рефлексия и дополнительный контекст от инструментов передаются в промпт пересмотра.

-   Ответ обновляется, создаётся новая саморефлексия и новые предложения по использованию инструментов.

-   Процесс повторяется N раз, пока финальный ответ не будет возвращён пользователю.

### Поиск по дереву языковых агентов (Tree of Thoughts)

**Источник:** _Language Agent Tree Search Unifies Reasoning, Acting and Planning in Language Models_ (12/05/2023)

Использует LLM как агентов, функции ценности и оптимизаторы в рамках алгоритма поиска по дереву Монте-Карло.

**Поток работы:**

1. Пользовательский запрос поступает на вход.

2. Генерируется первоначальный ответ как корневой узел дерева (либо ответ, либо выполнение инструмента).

3. Промпт рефлексии генерирует:

    - рефлексию по результату,

    - оценку результата,

    - определение, найдено ли решение.

4. Генерируются дополнительные N кандидатов с учётом предыдущего вывода и рефлексии, дерево расширяется.

5. Промпт рефлексии оценивает и выставляет баллы каждому новому кандидату.

6. Обновляются оценки лучшей «траектории».

7. Из лучшего дочернего узла генерируются следующие N кандидатов, цикл повторяется.

8. Процесс продолжается до достижения достаточной оценки или максимальной глубины поиска.

### Планирование и выполнение (Plan-And-Execute)

**Источник:** _Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models_ (05/26/2023)

**Поток работы:**

1. Пользовательский запрос поступает на вход.

2. Начальный промпт планирования формирует пошаговый план выполнения запроса.

3. Первый шаг плана передаётся агенту для генерации или выполнения инструмента.

4. Исходный запрос, исходный план и результаты предыдущих шагов передаются в промпт перепланирования.

5. Перепланировщик либо обновляет план, либо возвращает результат пользователю.

6. Обновлённый шаг снова передаётся агенту.

7. Цикл повторяется N раз, пока перепланировщик не сочтёт ответ достаточным.

### ReWOO — рассуждение без наблюдения

**Источник:** _ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models_ (05/23/2023)

Комбинирует многошаговый планировщик с подстановкой переменных для эффективного использования инструментов, снижая потребление токенов и время выполнения.

**Поток выполнения:**

1. Пользовательский запрос поступает на вход.

2. Планировщик генерирует план в виде списка задач со специальными переменными-заглушками.

3. План разбирается, и каждый шаг выполняется LLM-агентом.

4. Результат каждого шага подставляется в переменные следующего шага и передаётся обратно агенту.

5. После выполнения всех шагов план и «доказательства» из выполнения инструментов передаются в промпт Solver.

6. Solver генерирует и возвращает финальный ответ пользователю.

### LLMCompiler

**Источник:** _An LLM Compiler for Parallel Function Calling_ (02/06/2024)

LLMCompiler использует идею ориентированного ациклического графа из компиляторного дизайна для автоматической генерации оптимизированной оркестрации параллельных вызовов функций в стиле ReAct.

**Ключевая идея:**

-   Использование направленного ациклического графа (DAG)

-   Автоматическая оптимизация порядка вызова инструментов

-   Стиль исполнения, похожий на ReAct

**Поток работы:**

1. Пользовательский запрос поступает на вход.

2. Планировщик генерирует список задач с переменными-заглушками для зависимостей и строками «мыслей» для рассуждений.

3. Модуль извлечения задач анализирует план и определяет межзадачные зависимости.

4. Независимые задачи отправляются исполнителю параллельно.

5. Результаты исполнителя возвращаются в модуль извлечения задач для разрешения зависимостей.

6. Цикл повторяется, пока план не будет выполнен.

7. Полный результат передаётся в промпт Joiner:

    - либо формируется финальный ответ,

    - либо добавляется новая «мысль» и план отправляется на перепланирование.

8. При необходимости создаётся продолжение плана (не полностью новый план).

9. Процесс повторяется, пока Joiner не определит, что информации достаточно для ответа пользователю.

---

## Сравнительное тестирование (Agent Testing)

**Запрос:**

> _Текущие тенденции в цифровом маркетинге для технологических компаний_

**Модель:** GPT‑4‑Turbo

| Архитектура               | Время выполнения | Токены |
| ------------------------- | ---------------- | ------ |
| Базовое размышление       | 118.99 с         | 18,106 |
| Актор–рефлектор           | 69.04 с          | 24,608 |
| Tree of Thoughts          | 29.52 с          | 8,493  |
| Планирование и выполнение | 24.72 с          | 2,922  |
| ReWOO                     | 21.64 с          | 5,828  |
| LLMCompiler               | 11.29 с          | 2,745  |

### Basic Reflection

Принудительная переработка заставляет количество токенов **увеличиваться**
при длинных входных данных, например, тексте веб-сайта.

### Language Agent Tree Search

Удивительно быстрый, несмотря на генерацию большого количества вариантов.

Может экспоненциально «разбегаться», если глубина поиска высокая.

Использование LLM в качестве оценщика может быть **очень недетерминированным**.

### Plan-And-Execute

Принудительное добавление этапа планирования делает процесс
**немного более эффективным**, чем последовательные ревизии.

Склонен генерировать **достаточно хорошие ответы быстрее**,
используя собственные указания.

### Reasoning without Observation (ReWOO)

Оптимизация агентов типа _plan-and-execute_ здесь **хорошо заметна**.

Больше токенов, но **меньше времени обработки**.

### LLMCompiler

Ещё более глубокие оптимизации делают архитектуру **очень быстрой**,

однако она **не настроена для генерации полноценных отчётов**.

## Примечания и ограничения

-   Это **не научное сравнение**

-   Эти агенты **не настроены** на выполнение одной и той же задачи, промпта или инструментов.

-   Это **не научное сравнение** и не определение «лучшей» архитектуры агента.

-   Ответы **выдумываются**, не основаны на использовании инструментов или внешних исследованиях.

-   Обработка медленная, использование токенов высокое.

-   Принудительные ревизии увеличивают число токенов при длинных входных данных (например, текст сайта).

-   Поиск по дереву может экспоненциально разрастаться при большой глубине.

-   Использование LLM как оценщика может быть недетерминированным.

-   Этап планирования делает процесс более эффективным, чем серия ревизий.

-   Архитектуры plan-and-execute показывают оптимизацию по времени, несмотря на большее число токенов.

-   Дополнительные оптимизации делают выполнение очень быстрым, но не ориентированным на полноценную генерацию отчётов.

## Источники

-   [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366)

-   [https://arxiv.org/abs/2310.04406](https://arxiv.org/abs/2310.04406)

-   [https://arxiv.org/abs/2305.04091](https://arxiv.org/abs/2305.04091)

-   [https://arxiv.org/abs/2305.18323](https://arxiv.org/abs/2305.18323)

-   [https://arxiv.org/abs/2312.04511](https://arxiv.org/abs/2312.04511)

## Об авторе

https://www.linkedin.com/in/sergeiparfenov/

Исследователь и инженер в области ИИ/машинного обучения, имеющий степень бакалавра в области компьютерных информационных систем (Санкт-Петербургский государственный университет финансов и экономики) и прошедший курс CS231n: Глубокое обучение для компьютерного зрения в Стэнфорде. Я разрабатываю, создаю и внедряю готовые к использованию решения в области машинного обучения/ИИ, которые преобразуют передовые исследования в измеримую бизнес-ценность.
